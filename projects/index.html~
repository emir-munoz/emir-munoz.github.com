---
layout: default
title: "Projects"
description: "Projects in which I am or have been involved"
group: navigation-03
name: projects
---

<p>This is a list of relevant projects and done works in which I am now or have been involved:</p>

<div class="bs-component">
	<div class="row">
                <div class="col-lg-12">
			<h3>μRaptor <small>A DOM based system with appetite for hCard elements</small></h3>
                        <h4>September 2014</h4>
			<p>μRaptor is a DOM-based method to extract hCard microformats from HTML pages stripped of microformat markup. μRaptor extracts DOM sub-trees, 
                        converts them into rules, and uses them to extract hCard microformats. Besides, we use co-occurring CSS classes to improve the overall precision. 
                        Results on train data show 0.96 precision and 0.83 F1 measure by considering only the most common tree patterns. Furthermore, we propose the 
                        adoption of additional constraint rules on the values of hCard elements to further improve the extraction.<br>
			<a href="https://github.com/emir-munoz/uraptor" target="_blank"><i class="fa fa-github fa-1g"></i> [github]</a>
			</p>
		</div>
                <div class="col-lg-12">
			<h3>Linked Data Patterns</h3>
                        <h4>August 2014</h4>
			<p>Linked Data (LD) datasets (e.g., DBpedia, Freebase) are used in many knowledge extraction tasks due to the high variety of domains
                        they cover. Unfortunately, many of these datasets do not provide a description for their properties and classes, reducing the users' freedom
                        to understand, reuse or enrich them. This work attempts to fill part of this lack by presenting an unsupervised approach to discover syntactic
                        patterns in the properties used in LD datasets. This approach produces a content patterns database generated from the textual data (content) of
                        properties, which describes the syntactic structures that each property have. Our analysis enables (i) a human-understanding of syntactic patterns
                        for properties in a LD dataset, and (ii) a structural description of properties that facilitates its reuse or extension. Results over DBpedia
                        dataset also show that our approach enables (iii) the detection of data inconsistencies, and (iv) the validation and suggestion of new values for
                        a property. We also outline how the resulting database can be exploited in several information extraction use cases.<br>
			<a href="https://github.com/emir-munoz/ld-patterns" target="_blank"><i class="fa fa-github fa-1g"></i> [github]</a>
			</p>
		</div>
		<div class="col-lg-12">
			<h3>Wikitables</h3>
                        <h4>2013</h4>
			<p>We propose that existing RDF knowledge-bases can be leveraged to extract facts (in the form of RDF triples) from relational HTML tables 
			on the Web with high accuracy. In particular, we propose methods using the DBpedia knowledge-base to extract facts from tables embedded 
			in Wikipedia articles (henceforth "Wikitables"), effectively enriching DBpedia with additional triples. We first survey the Wikitables 
			from a recent dump of Wikipedia to see how much raw data can potentially be exploited by our methods. We then propose methods to extract 
			RDF from these tables: we map table cells to DBpedia entities and, for cells in the same row, we isolate a set of candidate relationships 
			based on existing information in DBpedia for other rows. To improve accuracy, we investigate various machine learning methods to classify 
			extracted triples as correct or incorrect. We ultimately extract 7.9 million unique novel triples from one million Wikitables at an 
			estimated precision of 81.5%.<br>
			<a href="http://emunoz.org/wikitables" target="_blank"><i class="fa fa-external-link fa-1g"></i> [website]</a>
			<a href="https://github.com/emir-munoz/wikitables" target="_blank"><i class="fa fa-github fa-1g"></i> [github]</a>
			</p>
		</div>
		<div class="col-lg-12">
			<h3>LODPeas: Like peas in a LOD (cloud)</h3>
                        <h4>2012</h4>
			<p>We present <a href="http://lodpeas.org" target="_blank">LODPeas</a>: a system for browsing entities that are found to share many things 
			in common in an RDF dataset. The system first offers standard keyword search to locate a focus entity. Once a focus entity 
			has been found, other entities that share a lot in common with it are displayed in a graph-based visualisation. The degree 
			to which two entities have a lot in common---their level of concurrence---is scored by looking at attributes (property--value pairs) 
			that they share: attributes that are shared by few other entities are given higher weight, and additional shared attributes 
			imply a stronger score. LODPeas is designed to scale for billions of triples and is built in an (almost) entirely domain-agnostic 
			fashion, built on top of the RDF standards themselves and not requiring any domain-specific input. Herein, we describe the 
			functionality of LODPeas, how the system was built over the BTC'12, and discuss possible applications.<br>
			<a href="http://lodpeas.org" target="_blank"><i class="fa fa-external-link fa-1g"></i> [website]</a>
			<a href="http://code.google.com/p/lodpeas/" target="_blank"><i class="fa fa-code fa-1g"></i> [google-code]</a>
			</p>
		</div>
		<div class="col-lg-12">
			<h3>XML Constraints</h3>
                        <h4>2011</h4>
			<p>This was my master's thesis topic. In <a href="http://emunoz.org/xml-constraints/" target="_blank">this repository</a> you can be found
			some resources used in some submitted papers to different conferences. With the results of my thesis we submitted the paper 
			<a href="http://emunoz.org/publications/files/DEXA12.pdf" target="_blank">Performance Analysis of Algorithms to Reason about XML Keys</a>, 
			published in DEXA 2012, with which we earned the best paper award.<br>
			<a href="http://emunoz.org/xml-constraints/" target="_blank"><i class="fa fa-external-link fa-1g"></i> [website]</a>
			<a href="https://github.com/emir-munoz/xml-constraints" target="_blank"><i class="fa fa-github fa-1g"></i> [github]</a>
			</p>
		</div>
		<div class="col-lg-12">
			<h3>LaTex template for thesis USACH</h3>
                        <h4>2011</h4>
			<p>While I was doing my thesis I built a LaTex template following the official format from Universidad de Santiago de Chile. 
			This template can be downloaded <a href="https://github.com/emir-munoz/thesisLatex" target="_blank">here</a>. 
			If you like also can use an online LaTex compiler available <a href="https://www.sharelatex.com/" target="_blank">here</a>, 
			where you can upload the previous folder and compile using pdflatex.<br> 
			TODO: Upload the post-graduate version and generate a wiki with the steps to use these templates.<br>
			<a href="http://emunoz.org/tesis-usach" target="_blank"><i class="fa fa-external-link fa-1g"></i> [website]</a>
			<a href="https://www.sharelatex.com/project/5028d024c8e9697a1b064c07" target="_blank"><i class="fa fa-file-text fa-1g"></i> [sharelatex-pregrado]</a>
			<a href="https://www.sharelatex.com/project/51353d532c51654a28769220" target="_blank"><i class="fa fa-file-text fa-1g"></i> [sharelatex-postgrado]</a>
			</p>
		</div>
	</div>
</div>

{% comment %}<!--
<p>
When $a \ne 0$, there are two solutions to \(ax^2 + bx + c = 0\) and they are
$$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$
</p>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script>
<script type="text/javascript" src="/script/mathjax/MathJax.js?config=TeX-AMS_HTML-full"></script>
-->{% endcomment %}

